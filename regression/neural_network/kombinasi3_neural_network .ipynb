{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kombinasi 9 :\n",
    "* Delete Duplicate\n",
    "* Delete Null\n",
    "* Outlier capping winsorize sisi kanan\n",
    "* Encoding \n",
    "* Standard scaler\n",
    "* Feature selection -> K-Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Without Feature Selection/UFC_kombinasi9_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['B_Reach_cms'], axis=1)\n",
    "y = df['B_Reach_cms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../../regression_kaggle/UFC_kombinasi9_all_features.csv')\n",
    "df_test = df_test.drop(['B_Reach_cms'], axis=1, errors='ignore')\n",
    "df_test_id = df_test['id']\n",
    "df_test = df_test.drop(['id'], axis=1, errors='ignore')\n",
    "# Get the common columns between df and df_test\n",
    "common_columns = list(set(X.columns) & set(df_test.columns))\n",
    "# Update df_test to only include the common columns\n",
    "df_test = df_test[common_columns]\n",
    "X = X[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "# from keras import backend as K\n",
    "# from keras.regularizers import l1\n",
    "\n",
    "# # Define R-squared metric\n",
    "# def r_squared(y_true, y_pred):\n",
    "#     SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "#     SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "#     return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "# # Define the model architecture\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512, activation='relu', input_shape=(X.shape[1],), kernel_regularizer=l1(0.001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(256, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(128, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# # Compile the model\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r_squared])\n",
    "\n",
    "# # Define early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# # Define learning rate scheduler\n",
    "# def scheduler(epoch, lr):\n",
    "#   if epoch < 10:\n",
    "#     return lr\n",
    "#   else:\n",
    "#     return lr * tf.math.exp(-0.1)\n",
    "\n",
    "# lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X, y, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, r2_score = model.evaluate(X, y)\n",
    "# print(\"R-squared:\", r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 33342.7422 - r_squared: -429.0867 - val_loss: 33079.1328 - val_r_squared: -408.6971\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 32407.5547 - r_squared: -404.6611 - val_loss: 31833.7617 - val_r_squared: -392.9194\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 30305.1406 - r_squared: -387.2776 - val_loss: 29227.1504 - val_r_squared: -360.6837\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 26712.5371 - r_squared: -332.6939 - val_loss: 24780.1406 - val_r_squared: -305.5268\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 22011.0430 - r_squared: -276.2962 - val_loss: 19635.2031 - val_r_squared: -241.7576\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 16796.4453 - r_squared: -208.2934 - val_loss: 14333.8682 - val_r_squared: -176.0924\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 11830.9287 - r_squared: -145.8170 - val_loss: 9614.8408 - val_r_squared: -117.8860\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 7651.1704 - r_squared: -95.7137 - val_loss: 5828.5186 - val_r_squared: -70.9571\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 4533.1948 - r_squared: -57.1800 - val_loss: 3274.8782 - val_r_squared: -39.4771\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 2422.0837 - r_squared: -29.1903 - val_loss: 1656.9441 - val_r_squared: -19.4686\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 1198.5123 - r_squared: -14.0471 - val_loss: 735.5170 - val_r_squared: -8.0866\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 615.5140 - r_squared: -6.7227 - val_loss: 311.5906 - val_r_squared: -2.8542\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 362.5565 - r_squared: -3.6205 - val_loss: 130.3826 - val_r_squared: -0.6048\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 271.0506 - r_squared: -2.4226 - val_loss: 70.7079 - val_r_squared: 0.1306\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 223.5580 - r_squared: -1.8014 - val_loss: 50.6815 - val_r_squared: 0.3797\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 240.2579 - r_squared: -1.9857 - val_loss: 37.2406 - val_r_squared: 0.5569\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 207.6012 - r_squared: -1.6912 - val_loss: 36.7217 - val_r_squared: 0.5511\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 204.3862 - r_squared: -1.5279 - val_loss: 32.5896 - val_r_squared: 0.6124\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 201.9083 - r_squared: -1.5138 - val_loss: 32.4276 - val_r_squared: 0.6077\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 198.2630 - r_squared: -1.4848 - val_loss: 33.6200 - val_r_squared: 0.5935\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 197.0627 - r_squared: -1.4605 - val_loss: 32.0647 - val_r_squared: 0.6142\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 199.3926 - r_squared: -1.4597 - val_loss: 31.1861 - val_r_squared: 0.6263\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 201.1395 - r_squared: -1.5332 - val_loss: 28.8466 - val_r_squared: 0.6487\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 196.7183 - r_squared: -1.4561 - val_loss: 26.9515 - val_r_squared: 0.6765\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 195.4409 - r_squared: -1.4515 - val_loss: 29.3895 - val_r_squared: 0.6461\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 195.6575 - r_squared: -1.4468 - val_loss: 31.9157 - val_r_squared: 0.6133\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 184.9630 - r_squared: -1.2917 - val_loss: 27.5757 - val_r_squared: 0.6735\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 192.4413 - r_squared: -1.3889 - val_loss: 27.5911 - val_r_squared: 0.6752\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 181.1873 - r_squared: -1.2563 - val_loss: 24.8965 - val_r_squared: 0.7024\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 174.7889 - r_squared: -1.1587 - val_loss: 25.6940 - val_r_squared: 0.6945\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 185.0216 - r_squared: -1.3418 - val_loss: 26.2794 - val_r_squared: 0.6907\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 175.4461 - r_squared: -1.2307 - val_loss: 26.1772 - val_r_squared: 0.6884\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 181.9053 - r_squared: -1.3017 - val_loss: 25.8826 - val_r_squared: 0.6951\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 166.4022 - r_squared: -1.0942 - val_loss: 26.1046 - val_r_squared: 0.6870\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 174.2806 - r_squared: -1.1910 - val_loss: 27.1411 - val_r_squared: 0.6745\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 168.2491 - r_squared: -1.1393 - val_loss: 27.1366 - val_r_squared: 0.6701\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 173.9590 - r_squared: -1.1430 - val_loss: 28.4681 - val_r_squared: 0.6605\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 165.2100 - r_squared: -1.0983 - val_loss: 27.8661 - val_r_squared: 0.6656\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 166.7020 - r_squared: -1.0714 - val_loss: 28.7995 - val_r_squared: 0.6592\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 166.8885 - r_squared: -1.0515 - val_loss: 26.0549 - val_r_squared: 0.6873\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 166.4852 - r_squared: -1.0753 - val_loss: 24.0502 - val_r_squared: 0.7110\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 165.5958 - r_squared: -1.0873 - val_loss: 24.6157 - val_r_squared: 0.6994\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 170.7882 - r_squared: -1.1201 - val_loss: 25.9758 - val_r_squared: 0.6865\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 155.0457 - r_squared: -0.9277 - val_loss: 23.7143 - val_r_squared: 0.7120\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 162.2008 - r_squared: -1.0181 - val_loss: 23.6296 - val_r_squared: 0.7164\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 147.2973 - r_squared: -0.9098 - val_loss: 26.3990 - val_r_squared: 0.6738\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 155.5556 - r_squared: -0.9628 - val_loss: 27.3485 - val_r_squared: 0.6677\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 159.9615 - r_squared: -0.9865 - val_loss: 27.9618 - val_r_squared: 0.6574\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 153.4140 - r_squared: -1.0458 - val_loss: 27.1179 - val_r_squared: 0.6737\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 156.7522 - r_squared: -0.9792 - val_loss: 24.7156 - val_r_squared: 0.6990\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 152.7314 - r_squared: -0.9251 - val_loss: 25.2285 - val_r_squared: 0.6958\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 145.1600 - r_squared: -0.8126 - val_loss: 25.9679 - val_r_squared: 0.6823\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 145.4297 - r_squared: -0.8160 - val_loss: 24.3544 - val_r_squared: 0.7044\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 151.9752 - r_squared: -0.9000 - val_loss: 24.4721 - val_r_squared: 0.7067\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 153.1525 - r_squared: -0.9830 - val_loss: 22.7124 - val_r_squared: 0.7261\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 149.6114 - r_squared: -0.8825 - val_loss: 23.0432 - val_r_squared: 0.7289\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 147.9996 - r_squared: -0.9119 - val_loss: 25.3549 - val_r_squared: 0.7025\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 148.8884 - r_squared: -0.8771 - val_loss: 26.0647 - val_r_squared: 0.6828\n",
      "Epoch 59/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 143.9311 - r_squared: -0.7797 - val_loss: 25.4856 - val_r_squared: 0.6993\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 144.5012 - r_squared: -0.7839 - val_loss: 22.3560 - val_r_squared: 0.7289\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 145.9709 - r_squared: -0.8449 - val_loss: 22.1715 - val_r_squared: 0.7313\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 137.1251 - r_squared: -0.6866 - val_loss: 22.6501 - val_r_squared: 0.7320\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 143.0726 - r_squared: -0.7904 - val_loss: 22.4242 - val_r_squared: 0.7291\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 132.7953 - r_squared: -0.6301 - val_loss: 24.5927 - val_r_squared: 0.7069\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 143.3448 - r_squared: -0.7701 - val_loss: 23.2300 - val_r_squared: 0.7183\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 133.3410 - r_squared: -0.6657 - val_loss: 22.5402 - val_r_squared: 0.7254\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 140.7392 - r_squared: -0.7919 - val_loss: 22.1703 - val_r_squared: 0.7297\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 133.5959 - r_squared: -0.6375 - val_loss: 23.7491 - val_r_squared: 0.7130\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 135.0856 - r_squared: -0.6768 - val_loss: 22.6716 - val_r_squared: 0.7240\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 135.7104 - r_squared: -0.7207 - val_loss: 22.8104 - val_r_squared: 0.7270\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 138.4259 - r_squared: -0.7155 - val_loss: 23.6943 - val_r_squared: 0.7157\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 141.4570 - r_squared: -0.7847 - val_loss: 24.3162 - val_r_squared: 0.7106\n",
      "Epoch 73/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 133.6556 - r_squared: -0.6974 - val_loss: 21.9566 - val_r_squared: 0.7375\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 135.9884 - r_squared: -0.7030 - val_loss: 23.2717 - val_r_squared: 0.7220\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 131.5809 - r_squared: -0.6264 - val_loss: 22.0412 - val_r_squared: 0.7368\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 135.3850 - r_squared: -0.6782 - val_loss: 22.3327 - val_r_squared: 0.7332\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 134.9664 - r_squared: -0.6924 - val_loss: 21.6769 - val_r_squared: 0.7440\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 131.7225 - r_squared: -0.6723 - val_loss: 21.2574 - val_r_squared: 0.7501\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 132.6193 - r_squared: -0.6790 - val_loss: 23.9170 - val_r_squared: 0.7167\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 135.1616 - r_squared: -0.6933 - val_loss: 23.2375 - val_r_squared: 0.7274\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 123.8797 - r_squared: -0.5148 - val_loss: 23.1871 - val_r_squared: 0.7221\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 139.4747 - r_squared: -0.7477 - val_loss: 24.3068 - val_r_squared: 0.7152\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 131.2881 - r_squared: -0.6300 - val_loss: 23.2133 - val_r_squared: 0.7243\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 135.8177 - r_squared: -0.6994 - val_loss: 22.8284 - val_r_squared: 0.7291\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 122.8795 - r_squared: -0.5380 - val_loss: 23.0082 - val_r_squared: 0.7265\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 134.2835 - r_squared: -0.6738 - val_loss: 21.6548 - val_r_squared: 0.7450\n",
      "Epoch 87/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 135.4669 - r_squared: -0.6785 - val_loss: 24.6910 - val_r_squared: 0.7093\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 136.0331 - r_squared: -0.7088 - val_loss: 23.4536 - val_r_squared: 0.7272\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 129.5344 - r_squared: -0.6377 - val_loss: 21.4991 - val_r_squared: 0.7448\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 143.1089 - r_squared: -0.7947 - val_loss: 20.8819 - val_r_squared: 0.7563\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 134.9333 - r_squared: -0.6916 - val_loss: 19.2788 - val_r_squared: 0.7698\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 134.4565 - r_squared: -0.6719 - val_loss: 19.5994 - val_r_squared: 0.7673\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 128.9480 - r_squared: -0.6017 - val_loss: 22.6431 - val_r_squared: 0.7312\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 132.5391 - r_squared: -0.6823 - val_loss: 22.8874 - val_r_squared: 0.7315\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 129.1514 - r_squared: -0.5938 - val_loss: 20.6729 - val_r_squared: 0.7542\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 127.7271 - r_squared: -0.5926 - val_loss: 22.6284 - val_r_squared: 0.7348\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 129.6035 - r_squared: -0.5934 - val_loss: 23.3665 - val_r_squared: 0.7263\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 126.6640 - r_squared: -0.5887 - val_loss: 20.7968 - val_r_squared: 0.7558\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 128.7555 - r_squared: -0.5903 - val_loss: 20.1728 - val_r_squared: 0.7626\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 126.5472 - r_squared: -0.5943 - val_loss: 20.2951 - val_r_squared: 0.7624\n",
      "Epoch 101/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 124.4786 - r_squared: -0.5631 - val_loss: 20.2371 - val_r_squared: 0.7625\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 126.0304 - r_squared: -0.5810 - val_loss: 24.6818 - val_r_squared: 0.7096\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 133.4933 - r_squared: -0.7196 - val_loss: 19.8222 - val_r_squared: 0.7699\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 129.7708 - r_squared: -0.6254 - val_loss: 20.1005 - val_r_squared: 0.7681\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 124.1674 - r_squared: -0.5498 - val_loss: 21.2222 - val_r_squared: 0.7507\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 128.0039 - r_squared: -0.5858 - val_loss: 19.1292 - val_r_squared: 0.7763\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 119.2683 - r_squared: -0.4989 - val_loss: 19.2684 - val_r_squared: 0.7736\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 123.0952 - r_squared: -0.5311 - val_loss: 20.0043 - val_r_squared: 0.7637\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 123.5716 - r_squared: -0.5642 - val_loss: 21.3268 - val_r_squared: 0.7500\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 126.1538 - r_squared: -0.5866 - val_loss: 21.1051 - val_r_squared: 0.7512\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 124.2040 - r_squared: -0.5690 - val_loss: 21.3006 - val_r_squared: 0.7530\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 125.1470 - r_squared: -0.5564 - val_loss: 20.1837 - val_r_squared: 0.7651\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 124.9506 - r_squared: -0.5422 - val_loss: 21.4716 - val_r_squared: 0.7475\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 120.8519 - r_squared: -0.5648 - val_loss: 20.4612 - val_r_squared: 0.7588\n",
      "Epoch 115/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 126.8415 - r_squared: -0.5606 - val_loss: 20.4885 - val_r_squared: 0.7604\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 123.7409 - r_squared: -0.5341 - val_loss: 22.2103 - val_r_squared: 0.7348\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 126.5202 - r_squared: -0.6062 - val_loss: 20.3071 - val_r_squared: 0.7638\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 122.4283 - r_squared: -0.5500 - val_loss: 20.9509 - val_r_squared: 0.7544\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 119.8089 - r_squared: -0.4586 - val_loss: 20.4821 - val_r_squared: 0.7653\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 127.1888 - r_squared: -0.6191 - val_loss: 21.9942 - val_r_squared: 0.7485\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 122.4603 - r_squared: -0.5086 - val_loss: 21.4648 - val_r_squared: 0.7532\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 11.1793 - r_squared: 0.8755\n",
      "R-squared: 0.8755143284797668\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l1\n",
    "\n",
    "# Define R-squared metric\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],), activity_regularizer=l1(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu', activity_regularizer=l1(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu', activity_regularizer=l1(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r_squared])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, r2_score = model.evaluate(X, y)\n",
    "print(\"R-squared:\", r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "df_test_scaled = scaler.transform(df_test)\n",
    "y_pred = model.predict(df_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(y_pred, columns=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['id'] = df_test_id\n",
    "submission = predictions_df[['id', 'Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>166.826508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>169.806915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>186.004547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>169.545288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>174.530151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   Predicted\n",
       "0  0.0  166.826508\n",
       "1  1.0  169.806915\n",
       "2  2.0  186.004547\n",
       "3  3.0  169.545288\n",
       "4  4.0  174.530151"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('pred_kombinasi3_neural_network_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
