{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kombinasi 9 :\n",
    "* Delete Duplicate\n",
    "* Delete Null\n",
    "* Outlier capping winsorize sisi kanan\n",
    "* Encoding \n",
    "* Standard scaler\n",
    "* Feature selection -> K-Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Without Feature Selection/UFC_kombinasi9_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['B_Reach_cms'], axis=1)\n",
    "y = df['B_Reach_cms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../../regression_kaggle/UFC_kombinasi9_all_features.csv')\n",
    "df_test = df_test.drop(['B_Reach_cms'], axis=1, errors='ignore')\n",
    "df_test_id = df_test['id']\n",
    "df_test = df_test.drop(['id'], axis=1, errors='ignore')\n",
    "# Get the common columns between df and df_test\n",
    "common_columns = list(set(X.columns) & set(df_test.columns))\n",
    "# Update df_test to only include the common columns\n",
    "df_test = df_test[common_columns]\n",
    "X = X[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 33341.0547 - r_squared: -416.0704 - val_loss: 32985.9492 - val_r_squared: -407.2169 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 32398.9883 - r_squared: -406.0000 - val_loss: 31548.4688 - val_r_squared: -389.2720 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 30304.5195 - r_squared: -384.3315 - val_loss: 28830.6816 - val_r_squared: -355.6681 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 26738.9082 - r_squared: -338.1450 - val_loss: 24605.0176 - val_r_squared: -303.1652 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 22109.9590 - r_squared: -278.3349 - val_loss: 19298.2793 - val_r_squared: -237.2485 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 16885.4980 - r_squared: -206.5498 - val_loss: 14333.0938 - val_r_squared: -175.9033 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 11999.3252 - r_squared: -148.7441 - val_loss: 9577.0557 - val_r_squared: -116.9949 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 7770.4756 - r_squared: -95.2688 - val_loss: 5952.9058 - val_r_squared: -72.2431 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 4589.3813 - r_squared: -56.1315 - val_loss: 3260.1511 - val_r_squared: -38.9213 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 2498.7661 - r_squared: -30.9053 - val_loss: 1649.9235 - val_r_squared: -18.9769 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 1286.8159 - r_squared: -15.1938 - val_loss: 800.3322 - val_r_squared: -8.5314 - lr: 9.0484e-04\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 725.9227 - r_squared: -7.9610 - val_loss: 395.8462 - val_r_squared: -3.5156 - lr: 8.1873e-04\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 461.3135 - r_squared: -4.4832 - val_loss: 219.9487 - val_r_squared: -1.3617 - lr: 7.4082e-04\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 350.9054 - r_squared: -3.1411 - val_loss: 130.7299 - val_r_squared: -0.2536 - lr: 6.7032e-04\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 295.7171 - r_squared: -2.4715 - val_loss: 95.1998 - val_r_squared: 0.1768 - lr: 6.0653e-04\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 264.8430 - r_squared: -1.9839 - val_loss: 77.0997 - val_r_squared: 0.3954 - lr: 5.4881e-04\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 252.3177 - r_squared: -1.8461 - val_loss: 68.4196 - val_r_squared: 0.5071 - lr: 4.9659e-04\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 244.8794 - r_squared: -1.7552 - val_loss: 62.9497 - val_r_squared: 0.5715 - lr: 4.4933e-04\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 236.2946 - r_squared: -1.6113 - val_loss: 61.9217 - val_r_squared: 0.5865 - lr: 4.0657e-04\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 246.1154 - r_squared: -1.6903 - val_loss: 60.2803 - val_r_squared: 0.6062 - lr: 3.6788e-04\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 232.7156 - r_squared: -1.5226 - val_loss: 59.6042 - val_r_squared: 0.6129 - lr: 3.3287e-04\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 239.3757 - r_squared: -1.6856 - val_loss: 59.5869 - val_r_squared: 0.6094 - lr: 3.0119e-04\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 235.0219 - r_squared: -1.6004 - val_loss: 57.5544 - val_r_squared: 0.6388 - lr: 2.7253e-04\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 234.1675 - r_squared: -1.5931 - val_loss: 58.1481 - val_r_squared: 0.6278 - lr: 2.4660e-04\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 237.6317 - r_squared: -1.6351 - val_loss: 56.6802 - val_r_squared: 0.6457 - lr: 2.2313e-04\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 226.8825 - r_squared: -1.5490 - val_loss: 55.5301 - val_r_squared: 0.6594 - lr: 2.0190e-04\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 228.9998 - r_squared: -1.5442 - val_loss: 56.2058 - val_r_squared: 0.6490 - lr: 1.8268e-04\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 230.3479 - r_squared: -1.5365 - val_loss: 54.2623 - val_r_squared: 0.6698 - lr: 1.6530e-04\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 228.3285 - r_squared: -1.5404 - val_loss: 54.1764 - val_r_squared: 0.6722 - lr: 1.4957e-04\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 219.0022 - r_squared: -1.4025 - val_loss: 53.6925 - val_r_squared: 0.6770 - lr: 1.3534e-04\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 223.6333 - r_squared: -1.4641 - val_loss: 53.5513 - val_r_squared: 0.6791 - lr: 1.2246e-04\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 224.5185 - r_squared: -1.4736 - val_loss: 53.3615 - val_r_squared: 0.6814 - lr: 1.1080e-04\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 222.5027 - r_squared: -1.4033 - val_loss: 53.4419 - val_r_squared: 0.6803 - lr: 1.0026e-04\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 219.4827 - r_squared: -1.4030 - val_loss: 53.2956 - val_r_squared: 0.6821 - lr: 9.0718e-05\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 222.8687 - r_squared: -1.4491 - val_loss: 53.1442 - val_r_squared: 0.6844 - lr: 8.2085e-05\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 216.4733 - r_squared: -1.3508 - val_loss: 52.7903 - val_r_squared: 0.6885 - lr: 7.4273e-05\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.2857 - r_squared: -1.3725 - val_loss: 52.4509 - val_r_squared: 0.6920 - lr: 6.7205e-05\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 219.4339 - r_squared: -1.4241 - val_loss: 52.6409 - val_r_squared: 0.6894 - lr: 6.0810e-05\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 219.6647 - r_squared: -1.4397 - val_loss: 52.4574 - val_r_squared: 0.6899 - lr: 5.5023e-05\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 231.3166 - r_squared: -1.5671 - val_loss: 52.9004 - val_r_squared: 0.6852 - lr: 4.9787e-05\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 220.8758 - r_squared: -1.5103 - val_loss: 52.6327 - val_r_squared: 0.6877 - lr: 4.5049e-05\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 217.2385 - r_squared: -1.3588 - val_loss: 52.6342 - val_r_squared: 0.6880 - lr: 4.0762e-05\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 229.4619 - r_squared: -1.5466 - val_loss: 52.7177 - val_r_squared: 0.6857 - lr: 3.6883e-05\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 223.3865 - r_squared: -1.4409 - val_loss: 52.7736 - val_r_squared: 0.6842 - lr: 3.3373e-05\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 224.5471 - r_squared: -1.4886 - val_loss: 52.7164 - val_r_squared: 0.6837 - lr: 3.0197e-05\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 219.8982 - r_squared: -1.3783 - val_loss: 52.4065 - val_r_squared: 0.6882 - lr: 2.7324e-05\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 220.9949 - r_squared: -1.4556 - val_loss: 52.2855 - val_r_squared: 0.6904 - lr: 2.4723e-05\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 216.9980 - r_squared: -1.3750 - val_loss: 52.3183 - val_r_squared: 0.6896 - lr: 2.2371e-05\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 210.7319 - r_squared: -1.2761 - val_loss: 52.2850 - val_r_squared: 0.6901 - lr: 2.0242e-05\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 216.6819 - r_squared: -1.4055 - val_loss: 52.2760 - val_r_squared: 0.6904 - lr: 1.8316e-05\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 226.6441 - r_squared: -1.4837 - val_loss: 52.3283 - val_r_squared: 0.6891 - lr: 1.6573e-05\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 227.1973 - r_squared: -1.5280 - val_loss: 52.3292 - val_r_squared: 0.6896 - lr: 1.4996e-05\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 223.6501 - r_squared: -1.4862 - val_loss: 52.2236 - val_r_squared: 0.6908 - lr: 1.3569e-05\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 221.5026 - r_squared: -1.4799 - val_loss: 52.2156 - val_r_squared: 0.6907 - lr: 1.2277e-05\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 225.7549 - r_squared: -1.5007 - val_loss: 52.1983 - val_r_squared: 0.6910 - lr: 1.1109e-05\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 217.2521 - r_squared: -1.3738 - val_loss: 52.0980 - val_r_squared: 0.6920 - lr: 1.0052e-05\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 223.5314 - r_squared: -1.4430 - val_loss: 52.0272 - val_r_squared: 0.6930 - lr: 9.0953e-06\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 217.4639 - r_squared: -1.4502 - val_loss: 51.9680 - val_r_squared: 0.6936 - lr: 8.2297e-06\n",
      "Epoch 59/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 227.8923 - r_squared: -1.4911 - val_loss: 52.0088 - val_r_squared: 0.6930 - lr: 7.4466e-06\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 211.2558 - r_squared: -1.3147 - val_loss: 52.0263 - val_r_squared: 0.6928 - lr: 6.7379e-06\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 220.0246 - r_squared: -1.4139 - val_loss: 52.0438 - val_r_squared: 0.6926 - lr: 6.0967e-06\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 221.7592 - r_squared: -1.3815 - val_loss: 52.0009 - val_r_squared: 0.6929 - lr: 5.5165e-06\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 222.9021 - r_squared: -1.5028 - val_loss: 51.8830 - val_r_squared: 0.6944 - lr: 4.9916e-06\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 225.9550 - r_squared: -1.5092 - val_loss: 51.8838 - val_r_squared: 0.6944 - lr: 4.5166e-06\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 213.7804 - r_squared: -1.3632 - val_loss: 51.9099 - val_r_squared: 0.6942 - lr: 4.0868e-06\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 224.2927 - r_squared: -1.4821 - val_loss: 51.9294 - val_r_squared: 0.6939 - lr: 3.6979e-06\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 220.2166 - r_squared: -1.4064 - val_loss: 51.9595 - val_r_squared: 0.6935 - lr: 3.3460e-06\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 217.0558 - r_squared: -1.3857 - val_loss: 51.9488 - val_r_squared: 0.6938 - lr: 3.0275e-06\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 218.3607 - r_squared: -1.3833 - val_loss: 51.9337 - val_r_squared: 0.6938 - lr: 2.7394e-06\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 222.7559 - r_squared: -1.5456 - val_loss: 51.9964 - val_r_squared: 0.6930 - lr: 2.4787e-06\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 230.1408 - r_squared: -1.5468 - val_loss: 51.9336 - val_r_squared: 0.6939 - lr: 2.2429e-06\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 223.8942 - r_squared: -1.5077 - val_loss: 51.8933 - val_r_squared: 0.6940 - lr: 2.0294e-06\n",
      "Epoch 73/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 216.7552 - r_squared: -1.4479 - val_loss: 51.8670 - val_r_squared: 0.6947 - lr: 1.8363e-06\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 220.8493 - r_squared: -1.4621 - val_loss: 51.9424 - val_r_squared: 0.6934 - lr: 1.6616e-06\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 216.9738 - r_squared: -1.3595 - val_loss: 51.9048 - val_r_squared: 0.6937 - lr: 1.5034e-06\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.9934 - r_squared: -1.3289 - val_loss: 51.9492 - val_r_squared: 0.6933 - lr: 1.3604e-06\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 211.9108 - r_squared: -1.2925 - val_loss: 51.9597 - val_r_squared: 0.6930 - lr: 1.2309e-06\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 219.7025 - r_squared: -1.4144 - val_loss: 51.9205 - val_r_squared: 0.6934 - lr: 1.1138e-06\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 225.6973 - r_squared: -1.4518 - val_loss: 51.8792 - val_r_squared: 0.6940 - lr: 1.0078e-06\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 224.9658 - r_squared: -1.4861 - val_loss: 51.8532 - val_r_squared: 0.6944 - lr: 9.1188e-07\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 213.8691 - r_squared: -1.3508 - val_loss: 51.9337 - val_r_squared: 0.6932 - lr: 8.2510e-07\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 216.1539 - r_squared: -1.3721 - val_loss: 51.8578 - val_r_squared: 0.6938 - lr: 7.4658e-07\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 221.4799 - r_squared: -1.4425 - val_loss: 51.9065 - val_r_squared: 0.6934 - lr: 6.7554e-07\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 216.0395 - r_squared: -1.3976 - val_loss: 51.8840 - val_r_squared: 0.6939 - lr: 6.1125e-07\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 224.0522 - r_squared: -1.5225 - val_loss: 51.8826 - val_r_squared: 0.6939 - lr: 5.5308e-07\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 217.3363 - r_squared: -1.4785 - val_loss: 51.8963 - val_r_squared: 0.6938 - lr: 5.0045e-07\n",
      "Epoch 87/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 219.7601 - r_squared: -1.4276 - val_loss: 51.7901 - val_r_squared: 0.6950 - lr: 4.5283e-07\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 205.3178 - r_squared: -1.2732 - val_loss: 51.8616 - val_r_squared: 0.6940 - lr: 4.0973e-07\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 209.5334 - r_squared: -1.2794 - val_loss: 51.8876 - val_r_squared: 0.6936 - lr: 3.7074e-07\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 227.1158 - r_squared: -1.5329 - val_loss: 51.9193 - val_r_squared: 0.6933 - lr: 3.3546e-07\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 226.9036 - r_squared: -1.5195 - val_loss: 51.9177 - val_r_squared: 0.6935 - lr: 3.0354e-07\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 227.8871 - r_squared: -1.5513 - val_loss: 51.9078 - val_r_squared: 0.6936 - lr: 2.7465e-07\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 211.2902 - r_squared: -1.2797 - val_loss: 51.8374 - val_r_squared: 0.6942 - lr: 2.4852e-07\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 224.3367 - r_squared: -1.5177 - val_loss: 51.8455 - val_r_squared: 0.6943 - lr: 2.2487e-07\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 212.0430 - r_squared: -1.3339 - val_loss: 51.8083 - val_r_squared: 0.6946 - lr: 2.0347e-07\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 225.3692 - r_squared: -1.4932 - val_loss: 51.7971 - val_r_squared: 0.6948 - lr: 1.8410e-07\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.1087 - r_squared: -1.3442 - val_loss: 51.8566 - val_r_squared: 0.6943 - lr: 1.6659e-07\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 226.5891 - r_squared: -1.5006 - val_loss: 51.9464 - val_r_squared: 0.6932 - lr: 1.5073e-07\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 223.2231 - r_squared: -1.4337 - val_loss: 51.8157 - val_r_squared: 0.6947 - lr: 1.3639e-07\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 228.4035 - r_squared: -1.5215 - val_loss: 51.8937 - val_r_squared: 0.6938 - lr: 1.2341e-07\n",
      "Epoch 101/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 214.5343 - r_squared: -1.3872 - val_loss: 51.8309 - val_r_squared: 0.6947 - lr: 1.1167e-07\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 217.3762 - r_squared: -1.4246 - val_loss: 51.7695 - val_r_squared: 0.6955 - lr: 1.0104e-07\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 223.5326 - r_squared: -1.4575 - val_loss: 51.9373 - val_r_squared: 0.6934 - lr: 9.1424e-08\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 218.5600 - r_squared: -1.3718 - val_loss: 51.8496 - val_r_squared: 0.6942 - lr: 8.2724e-08\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 223.6142 - r_squared: -1.5148 - val_loss: 51.8353 - val_r_squared: 0.6945 - lr: 7.4851e-08\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.2998 - r_squared: -1.3749 - val_loss: 51.9117 - val_r_squared: 0.6936 - lr: 6.7728e-08\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 228.3767 - r_squared: -1.5485 - val_loss: 51.9453 - val_r_squared: 0.6933 - lr: 6.1283e-08\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 216.5262 - r_squared: -1.4071 - val_loss: 51.8188 - val_r_squared: 0.6947 - lr: 5.5451e-08\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 222.0148 - r_squared: -1.5241 - val_loss: 51.8974 - val_r_squared: 0.6938 - lr: 5.0174e-08\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 212.9000 - r_squared: -1.2874 - val_loss: 51.8981 - val_r_squared: 0.6937 - lr: 4.5400e-08\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 207.7099 - r_squared: -1.2943 - val_loss: 51.8709 - val_r_squared: 0.6941 - lr: 4.1079e-08\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.6400 - r_squared: -1.3611 - val_loss: 51.8913 - val_r_squared: 0.6939 - lr: 3.7170e-08\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 220.6817 - r_squared: -1.4150 - val_loss: 51.8815 - val_r_squared: 0.6939 - lr: 3.3633e-08\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 218.0539 - r_squared: -1.4623 - val_loss: 51.9007 - val_r_squared: 0.6933 - lr: 3.0432e-08\n",
      "Epoch 115/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.1878 - r_squared: -1.3481 - val_loss: 51.8786 - val_r_squared: 0.6939 - lr: 2.7536e-08\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 216.4434 - r_squared: -1.3454 - val_loss: 51.8206 - val_r_squared: 0.6947 - lr: 2.4916e-08\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 220.0313 - r_squared: -1.3988 - val_loss: 51.9024 - val_r_squared: 0.6936 - lr: 2.2545e-08\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.1453 - r_squared: -1.3361 - val_loss: 51.9859 - val_r_squared: 0.6926 - lr: 2.0399e-08\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 226.6268 - r_squared: -1.5025 - val_loss: 51.9861 - val_r_squared: 0.6925 - lr: 1.8458e-08\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 225.6590 - r_squared: -1.5004 - val_loss: 51.9034 - val_r_squared: 0.6937 - lr: 1.6702e-08\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 211.7420 - r_squared: -1.3219 - val_loss: 51.9207 - val_r_squared: 0.6935 - lr: 1.5112e-08\n",
      "Epoch 122/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 214.1380 - r_squared: -1.3727 - val_loss: 51.9252 - val_r_squared: 0.6935 - lr: 1.3674e-08\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 40.0516 - r_squared: 0.8339\n",
      "R-squared: 0.8339043259620667\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l1\n",
    "\n",
    "# Define R-squared metric\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X.shape[1],), kernel_regularizer=l1(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r_squared])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 10:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, r2_score = model.evaluate(X, y)\n",
    "print(\"R-squared:\", r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras import backend as K\n",
    "# from keras.regularizers import l1\n",
    "\n",
    "# # Define R-squared metric\n",
    "# def r_squared(y_true, y_pred):\n",
    "#     SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "#     SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "#     return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "# # Define the model architecture\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512, activation='relu', input_shape=(X.shape[1],), activity_regularizer=l1(0.001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(256, activation='relu', activity_regularizer=l1(0.001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(128, activation='relu', activity_regularizer=l1(0.001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# # Compile the model\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[r_squared])\n",
    "\n",
    "# # Define early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X, y, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, r2_score = model.evaluate(X, y)\n",
    "# print(\"R-squared:\", r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "df_test_scaled = scaler.transform(df_test)\n",
    "y_pred = model.predict(df_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(y_pred, columns=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['id'] = df_test_id\n",
    "submission = predictions_df[['id', 'Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>158.396530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>161.924149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>189.968018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>165.441910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>169.253510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   Predicted\n",
       "0  0.0  158.396530\n",
       "1  1.0  161.924149\n",
       "2  2.0  189.968018\n",
       "3  3.0  165.441910\n",
       "4  4.0  169.253510"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('pred_kombinasi3_neural_network_test2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
